{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dac8cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3transfer                 0.13.0\n",
      "transformers               4.55.2\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep tran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e209fcb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Image format not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m predictor = SAM2ImagePredictor.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mfacebook/sam2-hiera-large\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.inference_mode(), torch.autocast(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m, dtype=torch.bfloat16):\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[43mpredictor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_image\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/purestorage/AILAB/AI_1/tyk/3_CUProjects/VLMs/exercises/qwen25vl/assets/universal_recognition/unireco_birds_example.jpg\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     masks, _, _ = predictor.predict(\u001b[33m'\u001b[39m\u001b[33mbird\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/sam2/sam2_image_predictor.py:108\u001b[39m, in \u001b[36mSAM2ImagePredictor.set_image\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m    106\u001b[39m     \u001b[38;5;28mself\u001b[39m._orig_hw = [(h, w)]\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mImage format not supported\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    110\u001b[39m input_image = \u001b[38;5;28mself\u001b[39m._transforms(image)\n\u001b[32m    111\u001b[39m input_image = input_image[\u001b[38;5;28;01mNone\u001b[39;00m, ...].to(\u001b[38;5;28mself\u001b[39m.device)\n",
      "\u001b[31mNotImplementedError\u001b[39m: Image format not supported"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "predictor = SAM2ImagePredictor.from_pretrained(\"facebook/sam2-hiera-large\")\n",
    "\n",
    "with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    predictor.set_image('/purestorage/AILAB/AI_1/tyk/3_CUProjects/VLMs/exercises/qwen25vl/assets/universal_recognition/unireco_birds_example.jpg')\n",
    "    masks, _, _ = predictor.predict('bird')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0584077",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Sam2VideoModel' from 'transformers' (/opt/conda/lib/python3.11/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sam2VideoModel, Sam2VideoProcessor\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      4\u001b[39m device = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'Sam2VideoModel' from 'transformers' (/opt/conda/lib/python3.11/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import Sam2VideoModel, Sam2VideoProcessor\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Sam2VideoModel.from_pretrained(\"facebook/sam2-hiera-large\").to(device, dtype=torch.bfloat16)\n",
    "processor = Sam2VideoProcessor.from_pretrained(\"facebook/sam2-hiera-large\")\n",
    "\n",
    "# Load video frames (example assumes you have a list of PIL Images)\n",
    "# video_frames = [Image.open(f\"frame_{i:05d}.jpg\") for i in range(num_frames)]\n",
    "\n",
    "# For this example, we'll use the video loading utility\n",
    "from transformers.video_utils import load_video\n",
    "video_url = \"https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/bedroom.mp4\"\n",
    "video_frames, _ = load_video(video_url)\n",
    "\n",
    "# Initialize video inference session\n",
    "inference_session = processor.init_video_session(\n",
    "    video=video_frames,\n",
    "    inference_device=device,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Add click on first frame to select object\n",
    "ann_frame_idx = 0\n",
    "ann_obj_id = 1\n",
    "points = [[[[210, 350]]]]\n",
    "labels = [[[1]]]\n",
    "\n",
    "processor.add_inputs_to_inference_session(\n",
    "    inference_session=inference_session,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_ids=ann_obj_id,\n",
    "    input_points=points,\n",
    "    input_labels=labels,\n",
    ")\n",
    "\n",
    "# Segment the object on the first frame\n",
    "outputs = model(\n",
    "    inference_session=inference_session,\n",
    "    frame_idx=ann_frame_idx,\n",
    ")\n",
    "video_res_masks = processor.post_process_masks(\n",
    "    [outputs.pred_masks], original_sizes=[[inference_session.video_height, inference_session.video_width]], binarize=False\n",
    ")[0]\n",
    "print(f\"Segmentation shape: {video_res_masks.shape}\")\n",
    "\n",
    "# Propagate through the entire video\n",
    "video_segments = {}\n",
    "for sam2_video_output in model.propagate_in_video_iterator(inference_session):\n",
    "    video_res_masks = processor.post_process_masks(\n",
    "        [sam2_video_output.pred_masks], original_sizes=[[inference_session.video_height, inference_session.video_width]], binarize=False\n",
    "    )[0]\n",
    "    video_segments[sam2_video_output.frame_idx] = video_res_masks\n",
    "\n",
    "print(f\"Tracked object through {len(video_segments)} frames\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69d9a448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.53.1)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.55.2-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading transformers-4.55.2-py3-none-any.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: huggingface-hub, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.32.4\n",
      "    Uninstalling huggingface-hub-0.32.4:\n",
      "      Successfully uninstalled huggingface-hub-0.32.4\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.53.1\n",
      "    Uninstalling transformers-4.53.1:\n",
      "      Successfully uninstalled transformers-4.53.1\n",
      "Successfully installed huggingface-hub-0.34.4 transformers-4.55.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
