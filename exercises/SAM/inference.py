import torch, numpy as np, cv2
from PIL import Image
from transformers import AutoProcessor, GroundingDinoForObjectDetection
from transformers import SamProcessor, SamModel

img_path = "market.png"  # ë§ˆíŠ¸ ì´ë¯¸ì§€
image = Image.open(img_path).convert("RGB")
W, H = image.size

# 1) ì˜¤í”ˆì–´íœ˜ ë°•ìŠ¤ íƒì§€ (Grounding DINO)
# ë” êµ¬ì²´ì ì´ê³  íš¨ê³¼ì ì¸ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
text_prompt = "person . wheeled shopping trolley ."
proc_det = AutoProcessor.from_pretrained("IDEA-Research/grounding-dino-base")   # ë¹ ë¦„
det_model = GroundingDinoForObjectDetection.from_pretrained("IDEA-Research/grounding-dino-base").eval()

inputs = proc_det(images=image, text=text_prompt, return_tensors="pt")
with torch.no_grad():
    det_out = det_model(**inputs)

# post-process â†’ (x1,y1,x2,y2) in pixels
targets = torch.tensor([[H, W]])
results = proc_det.post_process_grounded_object_detection(
    det_out, inputs["input_ids"],
    box_threshold=0.15, text_threshold=0.15,  # threshold ë†’ì—¬ì„œ ê¹”ë”í•˜ê²Œ
    target_sizes=targets
)[0]

boxes = results["boxes"]               # (N,4), float32
labels = results["labels"]             # list[str]
scores = results["scores"]             # (N,)

# ì›ë³¸ íƒì§€ ê²°ê³¼ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
print(f"ğŸ” ì›ë³¸ íƒì§€ ê²°ê³¼:")
for i, (label, score) in enumerate(zip(labels, scores)):
    print(f"  {i}: '{label}' (ì‹ ë¢°ë„: {score:.3f})")

# ì‚¬ëŒ/ì¹´íŠ¸ë§Œ ë‚¨ê¸°ê¸° (grounding-dino-baseì˜ ë³µí•© ë¼ë²¨ì— ë§ì¶° í¬í•¨ ê´€ê³„ë¡œ í™•ì¸)
target_keywords = ["person", "wheeled", "shopping", "trolley"]
keep = [i for i,l in enumerate(labels) if any(keyword in l.lower() for keyword in target_keywords) and scores[i] > 0.30]  # base ëª¨ë¸ì€ ì‹ ë¢°ë„ ì ë‹¹íˆ
boxes = boxes[keep]; labels = [labels[i] for i in keep]; scores = scores[keep]

print(f"ğŸ“‹ í•„í„°ë§ í›„ ê²°ê³¼:")
for i, (label, score) in enumerate(zip(labels, scores)):
    print(f"  {i}: '{label}' (ì‹ ë¢°ë„: {score:.3f})")

# ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ê°„ë‹¨í•œ NMS ì ìš©
def simple_nms(boxes, scores, labels, iou_threshold=0.5):
    """ê°„ë‹¨í•œ NMS êµ¬í˜„"""
    if len(boxes) == 0:
        return [], [], []
    
    # ì‹ ë¢°ë„ ìˆœìœ¼ë¡œ ì •ë ¬
    indices = torch.argsort(scores, descending=True)
    keep = []
    
    while len(indices) > 0:
        current = indices[0]
        keep.append(current.item())
        
        if len(indices) == 1:
            break
            
        # í˜„ì¬ ë°•ìŠ¤ì™€ ë‚˜ë¨¸ì§€ ë°•ìŠ¤ë“¤ì˜ IoU ê³„ì‚°
        current_box = boxes[current]
        other_boxes = boxes[indices[1:]]
        
        # IoU ê³„ì‚° (ê°„ë‹¨ ë²„ì „)
        iou_scores = []
        for other_box in other_boxes:
            x1 = max(current_box[0], other_box[0])
            y1 = max(current_box[1], other_box[1])
            x2 = min(current_box[2], other_box[2])
            y2 = min(current_box[3], other_box[3])
            
            if x2 > x1 and y2 > y1:
                intersection = (x2 - x1) * (y2 - y1)
                area1 = (current_box[2] - current_box[0]) * (current_box[3] - current_box[1])
                area2 = (other_box[2] - other_box[0]) * (other_box[3] - other_box[1])
                union = area1 + area2 - intersection
                iou = intersection / union if union > 0 else 0
            else:
                iou = 0
            iou_scores.append(iou)
        
        # IoUê°€ thresholdë³´ë‹¤ ë‚®ì€ ë°•ìŠ¤ë“¤ë§Œ ë‚¨ê¹€
        iou_scores = torch.tensor(iou_scores)
        indices = indices[1:][iou_scores < iou_threshold]
    
    return [boxes[i] for i in keep], [scores[i] for i in keep], [labels[i] for i in keep]

# NMS ì ìš©
if len(boxes) > 0:
    boxes_nms, scores_nms, labels_nms = simple_nms(boxes, scores, labels, iou_threshold=0.3)
    boxes = torch.stack(boxes_nms) if boxes_nms else torch.empty(0, 4)
    scores = torch.stack(scores_nms) if scores_nms else torch.empty(0)
    labels = labels_nms
    
    print(f"ğŸ¯ NMS ì ìš© í›„ ìµœì¢… ê²°ê³¼:")
    for i, (label, score) in enumerate(zip(labels, scores)):
        print(f"  {i}: '{label}' (ì‹ ë¢°ë„: {score:.3f})")

# ê²€ì¶œëœ ë°•ìŠ¤ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
if len(boxes) == 0:
    print("ê²€ì¶œëœ ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤.")
    exit()

# 2) ë°•ìŠ¤ â†’ SAM ë§ˆìŠ¤í¬
sam_proc = SamProcessor.from_pretrained("facebook/sam-vit-huge")   # hugeë„ ê°€ëŠ¥í•˜ë‚˜ VRAMâ†‘
sam_model = SamModel.from_pretrained("facebook/sam-vit-huge").eval()

# SAMì€ [B, num_boxes, 4] ëª¨ì–‘ì˜ box(í”½ì…€ ë‹¨ìœ„, XYXY)ë¥¼ ê¸°ëŒ€
# boxes í…ì„œë¥¼ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜
boxes_list = boxes.tolist()
inputs_sam = sam_proc(image, input_boxes=[boxes_list], return_tensors="pt")
with torch.no_grad():
    sam_out = sam_model(**inputs_sam)

masks = sam_proc.post_process_masks(
    sam_out.pred_masks,        # [B, num_boxes, 1, H/4, W/4]
    inputs_sam["original_sizes"],
    inputs_sam["reshaped_input_sizes"]
)[0]                            # â†’ [num_boxes, H, W] bool/float

# 3) ì‹œê°í™”
img = cv2.imread(img_path)[:, :, ::-1]
overlay = img.copy()
print(f"ì´ë¯¸ì§€ í¬ê¸°: {img.shape}")
print(f"ë§ˆìŠ¤í¬ ê°œìˆ˜: {len(masks)}")

for i, m in enumerate(masks):
    print(f"ë§ˆìŠ¤í¬ {i} ì›ë³¸ í¬ê¸°: {m.shape}")
    
    # PyTorch í…ì„œë¥¼ numpyë¡œ ë³€í™˜
    if hasattr(m, 'cpu'):
        mask_tensor = m.cpu().numpy()
    else:
        mask_tensor = np.array(m)
    
    # 3ì°¨ì› ë§ˆìŠ¤í¬ì¸ ê²½ìš° ì²« ë²ˆì§¸ ì±„ë„ë§Œ ì‚¬ìš©í•˜ê±°ë‚˜ í‰ê·  ì·¨í•¨
    if len(mask_tensor.shape) == 3:
        if mask_tensor.shape[0] == 3:  # [3, H, W] í˜•íƒœ
            mask = mask_tensor[0]  # ì²« ë²ˆì§¸ ì±„ë„ ì‚¬ìš©
        else:  # [H, W, 3] í˜•íƒœ
            mask = mask_tensor.mean(axis=2)  # í‰ê·  ì·¨í•¨
    else:
        mask = mask_tensor.squeeze()
    
    # boolean ë§ˆìŠ¤í¬ë¡œ ë³€í™˜
    mask = mask > 0.5
    print(f"ì²˜ë¦¬ëœ ë§ˆìŠ¤í¬ {i} í¬ê¸°: {mask.shape}")
    
    # ë§ˆìŠ¤í¬ì™€ ì´ë¯¸ì§€ í¬ê¸°ê°€ ë‹¤ë¥´ë©´ ë¦¬ì‚¬ì´ì¦ˆ
    if mask.shape != overlay.shape[:2]:
        print(f"ë¦¬ì‚¬ì´ì¦ˆ: {mask.shape} -> {overlay.shape[:2]}")
        mask = cv2.resize(mask.astype(np.uint8), (overlay.shape[1], overlay.shape[0]), interpolation=cv2.INTER_NEAREST).astype(bool)
    
    # ë§ˆì¼“ ì´ë¯¸ì§€ ê°ì²´ë³„ë¡œ ë‹¤ë¥¸ ìƒ‰ìƒ ì§€ì •
    label = labels[i].lower()
    if "person" in label:
        color = np.array([0, 255, 0], dtype=np.uint8)  # ë…¹ìƒ‰ (RGB)
    elif any(cart_word in label for cart_word in ["wheeled", "shopping", "cart"]):
        color = np.array([0, 0, 255], dtype=np.uint8)  # íŒŒë€ìƒ‰ (RGB)
    else:
        color = np.array([255, 0, 0], dtype=np.uint8)  # ê¸°ë³¸ ë¹¨ê°„ìƒ‰ (RGB)
    
    print(f"ë¼ë²¨ '{labels[i]}' -> ìƒ‰ìƒ: {color}")
    
    # ë§ˆìŠ¤í¬ê°€ Trueì¸ í”½ì…€ì—ë§Œ ìƒ‰ìƒ ì ìš©
    for c in range(3):  # RGB ê° ì±„ë„ë³„ë¡œ ì²˜ë¦¬
        overlay[mask, c] = (0.5 * overlay[mask, c] + 0.5 * color[c]).astype(np.uint8)
    
    # ë¼ë²¨ ê·¸ë¦¬ê¸° (ìƒ‰ìƒë„ ê°ì²´ë³„ë¡œ ë§ì¶¤)
    x1,y1,x2,y2 = boxes[i].tolist()
    # OpenCVëŠ” BGR ìˆœì„œë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ RGB -> BGRë¡œ ë³€í™˜
    color_bgr = (int(color[2]), int(color[1]), int(color[0]))
    cv2.rectangle(overlay, (int(x1),int(y1)), (int(x2),int(y2)), color_bgr, 2)
    cv2.putText(overlay, labels[i], (int(x1), max(0,int(y1)-5)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color_bgr, 2)

out = (0.6*img + 0.4*overlay).astype(np.uint8)
cv2.imwrite("seg_groundingdino_sam.png", out[:, :, ::-1])
